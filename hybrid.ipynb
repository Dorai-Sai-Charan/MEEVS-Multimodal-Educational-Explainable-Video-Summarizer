{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e298a642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Running Hybrid Summarization:   0%|          | 0/100 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running Hybrid Summarization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [13:55<00:00,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Average Scores (Hybrid Extractive + Abstractive on 100 Samples):\n",
      "ROUGE-1:     0.3285\n",
      "ROUGE-2:     0.0895\n",
      "ROUGE-L:     0.1907\n",
      "ROUGE-Lsum:  0.2796\n",
      "METEOR:      0.1946\n",
      "BERT Precision: 0.8456\n",
      "BERT Recall:    0.8276\n",
      "BERT F1 Score:  0.8363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy transformers datasets evaluate bert-score --quiet\n",
    "\n",
    "# ðŸ“š Imports\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "# âœ… Load trained BART model\n",
    "model_path = \"bart_arxiv_30k_1024_model\"  # Change this if needed\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# âœ… Load dataset (100 test samples)\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "test_data = dataset[\"test\"].select(range(100))\n",
    "\n",
    "# âœ… Evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# âœ… Storage\n",
    "rouge1s, rouge2s, rougel, rougelsum = [], [], [], []\n",
    "meteors = []\n",
    "bert_precisions, bert_recalls, bert_f1s = [], [], []\n",
    "\n",
    "# âœ… Loop over 100 samples\n",
    "for sample in tqdm(test_data, desc=\"Running Hybrid Summarization\"):\n",
    "    article = sample[\"article\"]\n",
    "    reference = sample[\"abstract\"]\n",
    "\n",
    "    # --- Extractive Step ---\n",
    "    parser = PlaintextParser.from_string(article, Tokenizer(\"english\"))\n",
    "    top_sentences = TextRankSummarizer()(parser.document, 5)\n",
    "    extractive_summary = \" \".join(str(sent) for sent in top_sentences)\n",
    "\n",
    "    # --- Abstractive Step (BART) ---\n",
    "    inputs = tokenizer(extractive_summary, return_tensors=\"pt\", truncation=True, max_length=768)\n",
    "    summary_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=3,\n",
    "        length_penalty=2.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    pred = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    r = rouge.compute(predictions=[pred], references=[reference])\n",
    "    rouge1s.append(r[\"rouge1\"])\n",
    "    rouge2s.append(r[\"rouge2\"])\n",
    "    rougel.append(r[\"rougeL\"])\n",
    "    rougelsum.append(r[\"rougeLsum\"])\n",
    "\n",
    "    m = meteor.compute(predictions=[pred], references=[reference])\n",
    "    meteors.append(m[\"meteor\"])\n",
    "\n",
    "    b = bertscore.compute(predictions=[pred], references=[reference], lang=\"en\")\n",
    "    bert_precisions.append(b[\"precision\"][0])\n",
    "    bert_recalls.append(b[\"recall\"][0])\n",
    "    bert_f1s.append(b[\"f1\"][0])\n",
    "\n",
    "# âœ… Final Average Metrics\n",
    "print(\"\\nðŸ“Š Final Average Scores (Hybrid Extractive + Abstractive on 100 Samples):\")\n",
    "print(f\"ROUGE-1:     {sum(rouge1s)/100:.4f}\")\n",
    "print(f\"ROUGE-2:     {sum(rouge2s)/100:.4f}\")\n",
    "print(f\"ROUGE-L:     {sum(rougel)/100:.4f}\")\n",
    "print(f\"ROUGE-Lsum:  {sum(rougelsum)/100:.4f}\")\n",
    "print(f\"METEOR:      {sum(meteors)/100:.4f}\")\n",
    "print(f\"BERT Precision: {sum(bert_precisions)/100:.4f}\")\n",
    "print(f\"BERT Recall:    {sum(bert_recalls)/100:.4f}\")\n",
    "print(f\"BERT F1 Score:  {sum(bert_f1s)/100:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
